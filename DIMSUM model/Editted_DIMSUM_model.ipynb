{"cells":[{"cell_type":"markdown","id":"28cc5fd2","metadata":{},"source":["In our original dataset, there are 1000 files storing 1 million of playlists which cost around 30GB of storage. We preprocessed 10% of them with local spark and save in parquet format because parquet compresses the spark dataformat and make the data loading process faster. After multiple rounds of trial and erros, we found out that, with our GCP configuration, we are comfortable only with working on half of the 100 files saved in the folder. Hence, the following analysis will be based on this 50000 playlists which 5% of the entire dataset. Though it sounds very small, but this subset of data is 1.6 gb in size which, in our opinion, it aligns with the course's interest that is to study the distributed computation in big data anlaysis.  "]},{"cell_type":"code","execution_count":1,"id":"3e645f82","metadata":{},"outputs":[],"source":["path = \"gs://ncf446-201929129/rdd_data_metadata/\" "]},{"cell_type":"code","execution_count":42,"id":"3ea1b7c8","metadata":{},"outputs":[],"source":["import numpy as np \n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import time\n","import math\n","import random\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql import SQLContext\n","from pyspark.sql.functions import col, explode, count, monotonically_increasing_id\n","from pyspark.sql import Row\n","from pyspark.ml.feature import StringIndexer, OneHotEncoder\n","from pyspark.ml import Pipeline\n","from pyspark.sql.types import IntegerType\n","import pyspark.sql.functions as F\n","from pyspark.sql.functions import sum as _sum\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import MapType, StringType, DoubleType, FloatType"]},{"cell_type":"code","execution_count":3,"id":"01cbd5d1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]}],"source":["sqlContext = SQLContext(sc)"]},{"cell_type":"code","execution_count":4,"id":"2092b6ba","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 12:37:39 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"]}],"source":["spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\") # make the spark pandas dataframe cnversion mroe efficient "]},{"cell_type":"code","execution_count":5,"id":"69387dad","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#loading data\n","bigframe=[] \n","for i in range(1, 6, 1): #reading only 5 parquet files\n","    index = f'batch_{i}'\n","    fpath = path+index\n","    bigframe.append(spark.read.parquet(fpath)) "]},{"cell_type":"code","execution_count":6,"id":"97d4c141","metadata":{},"outputs":[],"source":["#merge all dataframe into one big dataframe\n","metaframe = bigframe[0]\n","for index, frame in enumerate(bigframe):\n","    if index>=len(bigframe)-1: #len = 5 index cannot be >5\n","        break\n","    else:\n","        metaframe = metaframe.union(bigframe[index+1])"]},{"cell_type":"code","execution_count":7,"id":"41a93dd6","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 5:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+-----+---------------+----------+----------+-------------+--------------------+\n","|  pid|           name|num_tracks|num_albums|num_followers|              tracks|\n","+-----+---------------+----------+----------+-------------+--------------------+\n","|10499|          Chill|        26|        24|            1|[{0, Heroin, null...|\n","|10500|         cardio|       146|       138|            1|[{0, Sweat (A La ...|\n","|10501|      Acoustic |        38|        38|            1|[{0, Hey - Acoust...|\n","|10502|        Its lit|        13|        13|            1|[{0, Faith - Radi...|\n","|10503|         Lounge|       168|       154|            1|[{0, Amazon Dawn,...|\n","|10504|Play it forward|        31|        29|            1|[{0, The General,...|\n","|10505|        Country|       116|        86|            2|[{0, Who Are You ...|\n","|10506|        country|        62|        53|            2|[{0, 19 You + Me,...|\n","|10507|   Chilllllllll|       106|        95|            2|[{0, Let Her Go, ...|\n","|10508|          Exit |        37|        37|            1|[{0, Keeping Your...|\n","+-----+---------------+----------+----------+-------------+--------------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["metaframe.show(10)"]},{"cell_type":"code","execution_count":8,"id":"96612416","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 6:===================================================>     (27 + 3) / 30]\r"]},{"name":"stdout","output_type":"stream","text":["number of palylists: 50000\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df = metaframe\n","print(f'number of palylists: {df.count()}')"]},{"cell_type":"code","execution_count":9,"id":"081dcf69","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Length of all playlists combined without removing duplicates: 3348258\n","+---+--------------------+------------+--------------------+--------------------+--------------------+--------------+--------------------+-----+\n","|pos|          track_name|track_artist|           track_uri|          album_name|           album_uri|   artist_name|          artist_uri|  pid|\n","+---+--------------------+------------+--------------------+--------------------+--------------------+--------------+--------------------+-----+\n","|  0|              Heroin|        null|spotify:track:3qo...|       Lust For Life|spotify:album:1nP...|  Lana Del Rey|spotify:artist:00...|10499|\n","|  1|Press Play and Es...|        null|spotify:track:1Fp...|Press Play and Es...|spotify:album:0io...|   Teflon Sega|spotify:artist:0J...|10499|\n","|  2|               Bones|        null|spotify:track:3xS...|            Bones EP|spotify:album:33R...|Dustin Tebbutt|spotify:artist:0z...|10499|\n","|  3|   Always In My Head|        null|spotify:track:0FM...|       Ghost Stories|spotify:album:2G4...|      Coldplay|spotify:artist:4g...|10499|\n","|  4|       Spotless Mind|        null|spotify:track:7zD...|          Souled Out|spotify:album:5wo...|    Jhene Aiko|spotify:artist:5Z...|10499|\n","|  5|   Over My Dead Body|        null|spotify:track:2Gn...|           Take Care|spotify:album:6X1...|         Drake|spotify:artist:3T...|10499|\n","|  6|            The Calm|        null|spotify:track:4tI...|         So Far Gone|spotify:album:61N...|         Drake|spotify:artist:3T...|10499|\n","|  7|Wasting My Young ...|        null|spotify:track:6Pm...|         If You Wait|spotify:album:4T8...|London Grammar|spotify:artist:3B...|10499|\n","|  8|Homegrown - Empty...|        null|spotify:track:48U...|           Homegrown|spotify:album:2x4...|          Haux|spotify:artist:1i...|10499|\n","|  9|            Two High|        null|spotify:track:0Z9...|            Two High|spotify:album:2X0...|     Moon Taxi|spotify:artist:5D...|10499|\n","+---+--------------------+------------+--------------------+--------------------+--------------------+--------------+--------------------+-----+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Length of all playlists combined with removing duplicates: 457016\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 19:=================================================>      (32 + 4) / 36]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|          track_name|           track_uri|          album_name|          album_name|         artist_name|          artist_uri|\n","+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|The Drying Of The...|spotify:track:6ly...|       The Wild Hunt|       The Wild Hunt|The Tallest Man O...|spotify:artist:2B...|\n","|     Finna Get Loose|spotify:track:160...|     Finna Get Loose|     Finna Get Loose|               Diddy|spotify:artist:59...|\n","|          Be In Love|spotify:track:4Ck...|              Better|              Better|   Chrisette Michele|spotify:artist:3Y...|\n","|              X-tasy|spotify:track:42H...|Miss E...So Addic...|Miss E...So Addic...|       Missy Elliott|spotify:artist:2w...|\n","|Someday At Christmas|spotify:track:4XT...| Under The Mistletoe| Under The Mistletoe|       Justin Bieber|spotify:artist:1u...|\n","|           Big Tymin|spotify:track:2Fd...|     Nef The Pharaoh|     Nef The Pharaoh|     Nef The Pharaoh|spotify:artist:3D...|\n","|Forever (feat. Al...|spotify:track:1WK...|          Wave Maker|          Wave Maker|                Tiny|spotify:artist:5N...|\n","|Helena (So Long &...|spotify:track:5dT...|Three Cheers For ...|Three Cheers For ...| My Chemical Romance|spotify:artist:7F...|\n","|              Runnin|spotify:track:0km...|                SDIB|                SDIB|                SDIB|spotify:artist:2M...|\n","|Coronation Band S...|spotify:track:5ta...|              Frozen|              Frozen|     Christophe Beck|spotify:artist:1G...|\n","+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df_playlistNtrack = df.select(col(\"tracks\"),col('pid')).withColumn(\"tracks\", explode(col(\"tracks\"))).select(\"tracks.*\", 'pid')\n","print(f'Length of all playlists combined without removing duplicates: {df_playlistNtrack.count()}')\n","df_playlistNtrack.show(10)\n","\n","distinct_tracks = df_playlistNtrack.select(col(\"track_name\"), col(\"track_uri\"), col(\"album_name\"), col(\"album_name\"), col(\"artist_name\"), col(\"artist_uri\")).distinct()\n","print(f'Length of all playlists combined with removing duplicates: {distinct_tracks.count()}')\n","distinct_tracks.show(10)"]},{"cell_type":"code","execution_count":10,"id":"00d6bc45","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 22:=======================================================>(59 + 1) / 60]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n","|          track_name|           track_uri|          album_name|          album_name|         artist_name|          artist_uri|unique_id|\n","+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n","|The Drying Of The...|spotify:track:6ly...|       The Wild Hunt|       The Wild Hunt|The Tallest Man O...|spotify:artist:2B...|        0|\n","|     Finna Get Loose|spotify:track:160...|     Finna Get Loose|     Finna Get Loose|               Diddy|spotify:artist:59...|        1|\n","|          Be In Love|spotify:track:4Ck...|              Better|              Better|   Chrisette Michele|spotify:artist:3Y...|        2|\n","|              X-tasy|spotify:track:42H...|Miss E...So Addic...|Miss E...So Addic...|       Missy Elliott|spotify:artist:2w...|        3|\n","|Someday At Christmas|spotify:track:4XT...| Under The Mistletoe| Under The Mistletoe|       Justin Bieber|spotify:artist:1u...|        4|\n","|           Big Tymin|spotify:track:2Fd...|     Nef The Pharaoh|     Nef The Pharaoh|     Nef The Pharaoh|spotify:artist:3D...|        5|\n","|Forever (feat. Al...|spotify:track:1WK...|          Wave Maker|          Wave Maker|                Tiny|spotify:artist:5N...|        6|\n","|Helena (So Long &...|spotify:track:5dT...|Three Cheers For ...|Three Cheers For ...| My Chemical Romance|spotify:artist:7F...|        7|\n","|              Runnin|spotify:track:0km...|                SDIB|                SDIB|                SDIB|spotify:artist:2M...|        8|\n","|Coronation Band S...|spotify:track:5ta...|              Frozen|              Frozen|     Christophe Beck|spotify:artist:1G...|        9|\n","+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["distinct_tracks_with_id = distinct_tracks.withColumn('unique_id', monotonically_increasing_id())\n","distinct_tracks_with_id.show(10)"]},{"cell_type":"markdown","id":"8ed43bae","metadata":{},"source":["Since we are only interested in modellling the relationship between item and item using implicit feedbacks data, the only useful information here is track_uri and playlist id which is known as pid above. The reason why only track_uri and playlist id are useful is because all other columns are just representation of those two columns ie they are perfectly correlated. Keep in mind that machine and model do not understand alphabet as what human do hence we need to make the track uri to integer using LabelEncoder from spark API. "]},{"cell_type":"code","execution_count":17,"id":"bd161362","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 12:50:13 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","[Stage 45:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+-----+--------------------+--------------+\n","|  pid|           track_uri|encoded_tracks|\n","+-----+--------------------+--------------+\n","|10499|spotify:track:3qo...|       25451.0|\n","|10499|spotify:track:1Fp...|      139062.0|\n","|10499|spotify:track:3xS...|       18628.0|\n","|10499|spotify:track:0FM...|       17239.0|\n","|10499|spotify:track:7zD...|        8548.0|\n","|10499|spotify:track:2Gn...|        4899.0|\n","|10499|spotify:track:4tI...|       31790.0|\n","|10499|spotify:track:6Pm...|        8057.0|\n","|10499|spotify:track:48U...|       89267.0|\n","|10499|spotify:track:0Z9...|        4730.0|\n","+-----+--------------------+--------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["pid_trackID = df_playlistNtrack.select(col('pid'), col('track_uri')) #only keep the useful information\n","indexer = StringIndexer(inputCol=\"track_uri\", outputCol=\"encoded_tracks\") \n","indexed_DF = indexer.fit(pid_trackID).transform(pid_trackID) \n","indexed_DF.show(10) "]},{"cell_type":"markdown","id":"c9e75680","metadata":{},"source":["co-occurance here serves as labels in the sense that the one represents the particular track exists in the particular playlist. "]},{"cell_type":"code","execution_count":18,"id":"a243c3eb","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["3348258\n"]},{"name":"stderr","output_type":"stream","text":["23/04/22 12:50:24 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","[Stage 49:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+-----+--------------+------------+\n","|  pid|encoded_tracks|co_occurance|\n","+-----+--------------+------------+\n","|10499|         25451|           1|\n","|10499|        139062|           1|\n","|10499|         18628|           1|\n","|10499|         17239|           1|\n","|10499|          8548|           1|\n","|10499|          4899|           1|\n","|10499|         31790|           1|\n","|10499|          8057|           1|\n","|10499|         89267|           1|\n","|10499|          4730|           1|\n","+-----+--------------+------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["indexed_DF = indexed_DF.select(col('pid'),col('encoded_tracks'), F.lit(1).alias('co_occurance'))\n","indexed_DF = indexed_DF.withColumn('encoded_tracks',col('encoded_tracks').cast('int'))\n","print(indexed_DF.count())\n","indexed_DF.show(10)"]},{"cell_type":"markdown","id":"efbbb1fa","metadata":{},"source":["## Modelling starts here: "]},{"cell_type":"markdown","id":"929b56a4","metadata":{},"source":["Treat playlist as user and tracks as items. Then, we will need build a user item matrix whose row represents user and column represents item. Since it is extremely inefficient to store such a large yet sparse matrix, we only consider another way to represent the matrix without loss of information. Since the matrix is made of 0 and 1, we only keep the coordinate of row and column of the matrix which have entries as 1. "]},{"cell_type":"markdown","id":"1c621b81","metadata":{},"source":["Normalise the interaction column with is the co_occurance column:  \n","This means we are normalising the whole user item matrix with respect to items. Since we want to compute the items similarity score, hence we will normalise each column of the matrix by its l2 norm because column represents item.   \n","L2 norm = $\\sqrt{x_1^2 + ... + x_n^2}$   \n","$x_i$ stands for every entry of different rows at a particular column.  \n","The code below can be understood as for every column, we compute the L2 norm then keep this outputs in new column called c. "]},{"cell_type":"code","execution_count":27,"id":"3303662a","metadata":{},"outputs":[],"source":["squareDF = indexed_DF.withColumn(\"sqval\", indexed_DF['co_occurance'] ** 2) \n","squareDF = squareDF.selectExpr(\"pid as rowidx\",\"encoded_tracks as colidx\" ,\"co_occurance\", \"sqval\")\n","summsquare = squareDF.groupBy('colidx').agg(_sum(\"sqval\").alias(\"c\"))\n","summsquare2 = summsquare.withColumn(\"c\", F.sqrt(\"c\"))  "]},{"cell_type":"code","execution_count":28,"id":"27009691","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 13:14:53 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:14:54 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","23/04/22 13:15:06 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","[Stage 61:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+------+------+------------------+\n","|rowidx|colidx|                 c|\n","+------+------+------------------+\n","|106163|432875|               1.0|\n","|137235|  3918|11.874342087037917|\n","|136572|  3918|11.874342087037917|\n","|134440|  3918|11.874342087037917|\n","|141898|  3918|11.874342087037917|\n","|141760|  3918|11.874342087037917|\n","|141385|  3918|11.874342087037917|\n","|139451|  3918|11.874342087037917|\n","|139271|  3918|11.874342087037917|\n","| 14775|  3918|11.874342087037917|\n","+------+------+------------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["new_df = squareDF.join(summsquare2, on='colidx').select(col('rowidx'), col('colidx'), col('c'))\n","new_df.show(10)"]},{"cell_type":"code","execution_count":30,"id":"f074646c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 13:33:07 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:33:23 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","23/04/22 13:33:28 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:33:38 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:33:39 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:33:48 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","23/04/22 13:33:50 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:33:58 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","                                                                                \r"]}],"source":["m = new_df.select(col('rowidx')).distinct().count() #count the number of users\n","n = new_df.select(col('colidx')).distinct().count() #count the number of items"]},{"cell_type":"markdown","id":"1a0f010e","metadata":{},"source":["The model is robust against the increase in number of rows in terms of efficiency and the paper discussed about the setting of matrix of shape n by m with n >> m. However, in our experiment, we cannot use the entire dataset of a million playlist. Hence, we will experiment with the opposite case instead. In practise, this is a very practical assumption because most of the companies will ahve more customers than products. "]},{"cell_type":"code","execution_count":31,"id":"4099ee6c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["number of users: 50000\n","number of items: 457016\n"]}],"source":["print(f'number of users: {m}')\n","print(f'number of items: {n}')"]},{"cell_type":"markdown","id":"8cdbd2f2","metadata":{},"source":["Compute the parameter sqrtgamma. \n","The formula is based on heuristics of the computation. As suggestion from the author[1], we may use:  \n","\n","$\\sqrt{\\gamma}$ = $\\sqrt{\\frac{4 \\log{(n)}}{s}}$   \n","\n","where s is similarity_threshold. \n","\n","**n**: the number of items. As the number of items increases, the complexity of compututation of pairwise similarities also increases. Hence, having a larg n should lead to a small c to limit the number of similarities.\n","\n","**similarity_threshold**: the minimum similarity value we are interested in. A high similarity threshold means that we want to be more selective with the item pairs we consider. Therefore, having a higher similarity threshold should lead to a smaller c.\n","\n","**4 * math.log(n)**: This term serves as a scaling factor. By incorporating the log of the number of items, the scaling factor grows slower than the number of items, maintaining the balance between the number of computed similarities and the probability of missing relevant similarities. \n","\n","[1]: https://blog.twitter.com/engineering/en_us/a/2014/all-pairs-similarity-via-dimsum "]},{"cell_type":"code","execution_count":33,"id":"ab5da690","metadata":{},"outputs":[],"source":["# start_time = time.time()\n","similarity_threshold = 0.1\n","sqrtgamma = math.sqrt(4 * math.log(n) / similarity_threshold)\n","adjlist = new_df.groupBy(\"rowidx\").agg(F.collect_list(F.struct(\"colidx\", \"c\")).alias(\"r_i\")) "]},{"cell_type":"code","execution_count":35,"id":"6149f143","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 13:42:44 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:42:45 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","23/04/22 13:43:02 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:43:07 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","[Stage 86:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+------+--------------------+\n","|rowidx|                 r_i|\n","+------+--------------------+\n","|    31|[{75131, 2.236067...|\n","|    34|[{4469, 11.090536...|\n","|    53|[{355, 25.9229627...|\n","|    65|[{882, 20.9045449...|\n","|    78|[{272711, 1.0}, {...|\n","|    85|[{747, 21.9544984...|\n","|   108|[{19216, 5.0}, {2...|\n","|   133|[{74662, 2.236067...|\n","|   137|[{1311, 18.384776...|\n","|   148|[{1837, 16.492422...|\n","+------+--------------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["adjlist.show(10)"]},{"cell_type":"markdown","id":"4a9a6203","metadata":{},"source":["### The mapper: LeanDIMSUMMapper"]},{"cell_type":"markdown","id":"197e8fc4","metadata":{},"source":["- Here, the objective is to compute the approximate pairwise similarities between items while reducing computational complexity through adaptive sampling technique.   \n","\n","- The pairwise similarity measure is a variant of cosine similairty which mathematically is the distance between two high dimensional vectors in a sparse vector space. To compute every paairwise similairty as what consine similarity recommendation model does, the algorithm here essentially try to reduce the number of item pairs for which similarities need to be computed by taking into account of the fact that the row is sparse hence more likely than not we are multiplying one with zero or zero with zero. Hence, the algorithm does not go through every possible pair of vectors in the space.   \n","\n","- It uses the L2-norm of the rows in the input matrix to calculate the sampling probability for each item pair. This sampling probability is proportional to the product of the row lengths for the two items being compared. Pairs with higher row lengths are more likely to have higher similarity and are thus sampled more frequently.   \n","\n","- r_i is a list of dictionaries that contains the column index and the value of the non-zero elements in the i-th row of the input matrix.   \n","\n","- we iterate over every dict in r_i to get the column index colidx_j and use that to calculate the sampling probability. we then randomly generate a number from uniform distribution $[0,1]$ between 0 and 1 is less than prob_j. If it is, then proceed to the inner loop. this mechanism has slight similarity as MCMC algorithm.   \n","\n","- In the inner loop, we iterate over every dictionary again in r_i to get the column index colidx_k and the value d for each non-zero element in the row. This time, if the prob_k is not larger than the random generated value then we will pass to new iteration. If it is larger than the value then compute the key and value for the emitted pair. The key is a string concatenation of colidx_j and colidx_k, separated by an underscore. \n","\n","- The value is calculated as the product of c and d divided by the product of the minimum values between sqrtgamma and c, and sqrtgamma and d. Update the emit dictionary with the computed key-value pair.  \n"]},{"cell_type":"code","execution_count":43,"id":"f324af1f","metadata":{},"outputs":[],"source":["def mapper(r_i):\n","    emit = {}\n","    for dict_j in r_i:\n","        colidx_j = dict_j['colidx']\n","        c = dict_j['c']\n","        prob_j = min(1.0, sqrtgamma/ c)\n","        if random.random() < prob_j:\n","            for dict_k in r_i: \n","                colidx_k = dict_k['colidx']\n","                d = dict_k['c'] \n","                prob_k = min(1.0, sqrtgamma/d)\n","                if random.random()< prob_k:\n","                    key = str(colidx_j)+'_'+str(colidx_k)\n","                    val = c * d/ (min(sqrtgamma, c)*min(sqrtgamma, d))\n","                    emit[key] = val\n","    return emit     "]},{"cell_type":"code","execution_count":44,"id":"d1980be3","metadata":{},"outputs":[],"source":["mapper_udf = F.udf(lambda rows: mapper(rows), MapType(StringType(), FloatType())) \n","adjlist1 = adjlist.withColumn('emit', mapper_udf(adjlist.r_i))"]},{"cell_type":"code","execution_count":45,"id":"97b9ea81","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 13:45:01 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:45:02 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","23/04/22 13:45:16 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:45:19 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","[Stage 100:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+------+--------------------+--------------------+\n","|rowidx|                 r_i|                emit|\n","+------+--------------------+--------------------+\n","|    31|[{623, 23.0}, {56...|{529_570 -> 1.074...|\n","|    34|[{1507, 17.578395...|{31744_2605 -> 1....|\n","|    53|[{8928, 7.7459666...|{1160_320116 -> 1...|\n","|    65|[{65, 34.13209633...|{126_2484 -> 1.35...|\n","|    78|[{3699, 12.206555...|{3699_11502 -> 1....|\n","|    85|[{392, 25.5147016...|{4612_189 -> 1.27...|\n","|   108|[{13832, 6.0}, {1...|{18811_19108 -> 1...|\n","|   133|[{5120, 10.295630...|{181842_3218 -> 1...|\n","|   137|[{6622, 9.0553851...|{4292_10585 -> 1....|\n","|   148|[{340, 26.1533936...|{84185_3438 -> 1....|\n","+------+--------------------+--------------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["adjlist1.show(10)"]},{"cell_type":"code","execution_count":47,"id":"7eefdb47","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 13:45:50 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:45:51 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","23/04/22 13:46:02 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:46:06 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","[Stage 107:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+------+--------------------+--------------------+---------+\n","|rowidx|                 r_i|                emit|num_emits|\n","+------+--------------------+--------------------+---------+\n","|    31|[{76, 33.68976105...|{529_570 -> 1.074...|     8869|\n","|    34|[{3127, 13.152946...|{31744_2605 -> 1....|     6091|\n","|    53|[{2007, 15.937377...|{1160_320116 -> 1...|     9253|\n","|    65|[{2484, 14.525839...|{29642_66091 -> 1...|    16527|\n","|    78|[{45087, 3.0}, {1...|{3699_11502 -> 1....|      256|\n","|    85|[{28, 38.03945320...|{546_376 -> 1.161...|     4129|\n","|   108|[{17147, 5.385164...|{18811_19108 -> 1...|     2500|\n","|   133|[{41449, 3.162277...|{181842_3218 -> 1...|      400|\n","|   137|[{8340, 8.0}, {42...|{105_5720 -> 1.39...|      532|\n","|   148|[{5526, 9.9498743...|{14903_90038 -> 1...|    19026|\n","+------+--------------------+--------------------+---------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["count_dicts = udf(lambda col: len(col.items()), IntegerType())\n","adjlistfilter = adjlist1.withColumn('num_emits', count_dicts(adjlist1['emit']))\n","adjlistfilter.show(10)"]},{"cell_type":"code","execution_count":48,"id":"e6cfade6","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 13:48:02 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:48:18 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","23/04/22 13:48:22 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:48:30 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","                                                                                \r"]},{"data":{"text/plain":["50000"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["adjlistfilter.count()  #to check with the following filtered result"]},{"cell_type":"code","execution_count":49,"id":"54c76c76","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 13:48:32 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:48:33 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","23/04/22 13:48:46 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:48:51 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","                                                                                \r"]},{"data":{"text/plain":["50000"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["adjlistfilter0 = adjlistfilter.filter('num_emits > 0')\n","adjlistfilter0.count() #we chose similarity_threshold to be quite small so it is normal to have the same counts. "]},{"cell_type":"code","execution_count":50,"id":"47d42bb2","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 13:51:04 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:51:05 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","23/04/22 13:51:17 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:51:22 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","[Stage 134:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+----------+---------+\n","|       key|    value|\n","+----------+---------+\n","|   529_570|1.0740683|\n","|8360_16770|      1.0|\n","| 2793_9991|      1.0|\n","| 6864_5052|      1.0|\n","| 462_36568|1.0799618|\n","|94969_1159|      1.0|\n","|  1805_692|      1.0|\n","|    462_90|  1.53999|\n","|   486_623|1.0744058|\n","|6757_36568|      1.0|\n","+----------+---------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["exploded = adjlistfilter.select(explode('emit'))\n","exploded.show(10)"]},{"cell_type":"markdown","id":"18983c76","metadata":{},"source":["### The reduce: summation reducer\n","\n","The reducer is less of a interesting mechanism as it is just a summation reducer. "]},{"cell_type":"code","execution_count":51,"id":"f4187350","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 13:51:26 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:51:27 WARN DAGScheduler: Broadcasting large task binary with size 27.9 MiB\n","23/04/22 13:51:38 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:51:41 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","23/04/22 13:57:06 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","[Stage 145:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+------------+-----------------+\n","|         key|       similarity|\n","+------------+-----------------+\n","|   4064_6422|              3.0|\n","|   5852_4064|              3.0|\n","|   20104_645|              3.0|\n","|350066_10683|              1.0|\n","| 10683_23754|              3.0|\n","|   4238_1261|              6.0|\n","|      843_19|74.26603364944458|\n","|     665_413|6.648103952407837|\n","|   5669_3747|              8.0|\n","|     424_483|69.40363001823425|\n","+------------+-----------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["edges = exploded.groupby(exploded['key']).agg(_sum('value').alias('similarity'))\n","edges.show(10) "]},{"cell_type":"markdown","id":"5a91ca5c","metadata":{},"source":["clean the key column and split them into two columns"]},{"cell_type":"code","execution_count":52,"id":"04cd2847","metadata":{},"outputs":[],"source":["edgesdf = edges.withColumn(\"key_split\", F.split(edges[\"key\"], \"_\"))  \n","edgesdf = edgesdf.withColumn(\"key1\", edgesdf[\"key_split\"].getItem(0)).withColumn(\"key2\", edgesdf[\"key_split\"].getItem(1))\n","edgesdf = edgesdf.drop(\"key_split\")"]},{"cell_type":"code","execution_count":53,"id":"4cfa64d1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 14:03:12 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","[Stage 156:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+-----------+------------------+-----+------+\n","|        key|        similarity| key1|  key2|\n","+-----------+------------------+-----+------+\n","|   377_1159|12.349004745483398|  377|  1159|\n","| 16841_1041|               3.0|16841|  1041|\n","|1041_187886|               1.0| 1041|187886|\n","| 16841_2403|               3.0|16841|  2403|\n","|   5199_377|15.716915130615234| 5199|   377|\n","|    462_720| 86.39694213867188|  462|   720|\n","|  1041_5254|               4.0| 1041|  5254|\n","|  16841_720|               4.0|16841|   720|\n","| 2232_39878|               1.0| 2232| 39878|\n","|43637_22340|               1.0|43637| 22340|\n","+-----------+------------------+-----+------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["edgesdf.show(10)"]},{"cell_type":"code","execution_count":54,"id":"ad3e2229","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 14:15:30 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","[Stage 167:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+-----------+------------------+-----+------+\n","|        key|        similarity| key1|  key2|\n","+-----------+------------------+-----+------+\n","|   377_1159|13.471641540527344|  377|  1159|\n","| 16841_1041|               3.0|16841|  1041|\n","|1041_187886|               1.0| 1041|187886|\n","| 16841_2403|               3.0|16841|  2403|\n","|   5199_377|14.594278335571289| 5199|   377|\n","|    462_720| 88.55686569213867|  462|   720|\n","|  1041_5254|               4.0| 1041|  5254|\n","|  16841_720|               4.0|16841|   720|\n","| 2232_39878|               1.0| 2232| 39878|\n","|43637_22340|               1.0|43637| 22340|\n","+-----------+------------------+-----+------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["edgesdf_filter = edgesdf.filter(edgesdf[\"key1\"] != edgesdf[\"key2\"]) #top similarity pairs are always the item itself to itself \n","edgesdf_filter.show(10) #hence filter it "]},{"cell_type":"code","execution_count":55,"id":"03be32cc","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 14:21:45 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","[Stage 178:=====================================================> (65 + 2) / 67]\r"]},{"name":"stdout","output_type":"stream","text":["+----+----+------------------+\n","|key1|key2|        similarity|\n","+----+----+------------------+\n","|   0|   4|1113.5994210243225|\n","|  38|   0|1091.8673872947693|\n","|   4|   0|1059.7823486328125|\n","|   0|  15|1022.0927059650421|\n","|   0|  38|1021.3159561157227|\n","|   0|  10|1013.8881661891937|\n","|  10|   0| 934.1441531181335|\n","|  15|   0| 908.5268497467041|\n","|   2|   5|  892.601514339447|\n","|   0|   6| 892.4115419387817|\n","|   9|   8| 880.1872181892395|\n","|  10|   4| 879.6758544445038|\n","|   5|   2|  873.849381685257|\n","|   4|  10| 872.5529730319977|\n","|   2|  12| 871.1944704055786|\n","|   6|   4| 869.7254869937897|\n","|   9|   0| 864.7741928100586|\n","|   4|   6| 862.4474494457245|\n","|  19|   0| 856.3447895050049|\n","|  15|  10| 854.0915157794952|\n","+----+----+------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["finaldf = edgesdf_filter.select(col('key1'), col('key2'), col('similarity')) \n","finaldf.orderBy(col('similarity').desc()).show(10) "]},{"cell_type":"markdown","id":"50b3cc32","metadata":{},"source":["there is a repetition of pair for eg, x_i & x_j and x_j & x_i are the same in the dataframe hence eliminate the duplicates. "]},{"cell_type":"code","execution_count":56,"id":"8540c612","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 14:27:15 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","[Stage 189:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+------+-----+------------------+\n","|  key1| key2|        similarity|\n","+------+-----+------------------+\n","|  4064| 6422|               3.0|\n","| 20104|  645|               3.0|\n","| 10683|23754|               3.0|\n","|   424|  483| 75.28529357910156|\n","|  3584| 5673|               1.0|\n","|  3747| 9926|               4.0|\n","|  1511| 3747|              10.0|\n","|  4275|  859|               2.0|\n","|   122| 1635| 85.53794574737549|\n","|111237| 1563|               1.0|\n","|111237| 5127|               1.0|\n","|  1757| 2489|               3.0|\n","|  1199|15463|               2.0|\n","|  1147|19415|               1.0|\n","| 12384| 7473|               1.0|\n","|   259|78889|1.2026582956314087|\n","|  2448|  640|12.019562244415283|\n","|  5518| 5667|               1.0|\n","| 38168|  780|               8.0|\n","|   309| 7927| 3.483818292617798|\n","+------+-----+------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["finaldf = finaldf.filter(col(\"key1\") < col(\"key2\"))  \n","finaldf.show(20)"]},{"cell_type":"markdown","id":"a81b9731","metadata":{},"source":["Final results:"]},{"cell_type":"code","execution_count":61,"id":"95b0c64d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/22 14:42:16 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n","[Stage 211:=================================================>     (36 + 4) / 40]\r"]},{"name":"stdout","output_type":"stream","text":["+----+----+-----------------+\n","|key1|key2|       similarity|\n","+----+----+-----------------+\n","|   0|  38|1128.822898864746|\n","|   0|   4| 976.986852645874|\n","|   0|  10| 975.914826631546|\n","|   0|  15| 926.843923330307|\n","|  10|   4|925.9745836257935|\n","|   2|   5| 892.601514339447|\n","|  10|  15|869.8496618270874|\n","|   0|  19|867.2305283546448|\n","|   8|   9|840.4795241355896|\n","|   4|   6|829.6962804794312|\n","|   0|   8|812.7971563339233|\n","|   0|   6| 791.530237197876|\n","|   3|  50|791.0237140655518|\n","|   2|   8|788.5979545116425|\n","|   5|   8| 786.571681022644|\n","|   0|   9|773.3443222045898|\n","|   1|  45|771.5560340881348|\n","|   5|   9|768.0380697250366|\n","|  12|   2| 760.509927034378|\n","|   6|   9|760.1692526340485|\n","+----+----+-----------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["finaldf.orderBy(col('similarity').desc()).show(20)"]},{"cell_type":"code","execution_count":57,"id":"6bcf61ca","metadata":{},"outputs":[],"source":["# end_time = time.time()"]},{"cell_type":"code","execution_count":58,"id":"e747eb91","metadata":{},"outputs":[],"source":["# time_used = end_time - start_time"]},{"cell_type":"code","execution_count":59,"id":"1d01c54a","metadata":{},"outputs":[],"source":["# tt = f\"time consumed when similarity = 0.1: {time_used}\\n\""]},{"cell_type":"markdown","id":"051c3785","metadata":{},"source":["The map reduce framework here is similar to but not the same as the map reduce mechanism in distributed computation in Apache spark. Here, we applied the MapReduce framework on a randomised algorithm to reduce the complexity of similarity index computation. The benefit of using the map reduce framework here is to make the computation more efficient as compare to wrapping the algorithm into one function and use apply function to apply the udf to every entry of the data column. The mechanism of splitting computation into two parts map and reduce makes the computation more efficient. "]},{"cell_type":"markdown","id":"34b6100b","metadata":{},"source":["To collect the time consumption statistics, we manually ran the entire script multiple times without using for loop to prevent crashing the kernel. After every iteration, we manually restarted the kernel in order to clean up the usage of the above computation in the memory of the master node to prevent computation overflow problem as 60gb of RAM is the minimum that able to run the script above. Without manually restart the kernel, there are some variables and data being saved in the RAM and once the loop trying to start the new iteration, for example, in the computation of adjlist, there will be 2 adjlist in the RAM before the assignment takes place and therefore crash the resource allocator. (This is our observations and understanding after many trial and errors). "]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}
